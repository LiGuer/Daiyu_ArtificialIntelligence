\iffalse
Copyright 2020 LiGuer. All Rights Reserved.
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
	http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================\fi
\documentclass{article} 
\usepackage{latexsym}
\usepackage{amssymb}
\usepackage[UTF8]{ctex}
\usepackage{geometry}\geometry{left=2cm, right=2cm, top=2cm, bottom=2cm}
\setlength{\parindent}{0pt}
\title{机器学习 Machine Learning}\author{华北电力大学--17级--熊梓豪}\date{}
\begin{document}
\maketitle
\tableofcontents
\newpage

\section{编程建议}
\subsection{在线练习题集}
PAT 甲级题集：https://www.patest.cn/practice\\
包括考试常见的算法和数据结构题目,把PAT 甲级题集题目做完，可以算入门\\
\textbf{PAT参考答案} 百度"\textbf{柳ruo}",她的博客和github上，有对每一道题，完整的解析和程序\\
· 如果对编程还不熟练，先练习"PAT 乙级题集"

\subsection{基础数据结构}
\textbf{线性表}: 数组，链表，栈，队列\\
\textbf{树}：树的三种遍历，完全二叉树的性质等\\
\textbf{图}：图的表达: 邻接表, 邻接矩阵

\subsection{基础算法}
\textbf{排序}：快速排序，堆排序，冒泡排序等\\
\textbf{遍历}：深度优先搜索，广度优先搜索\\
\textbf{图}： 最小生成树，最大流，迪杰斯特拉最短路搜索\\
动态规划、并查集，字符串训练 等

\subsection{STL 标准模板库}
STL 标准模板库的练习使用,vector, map, set, sort() 等 

\subsection{书籍推荐}
$[1]$ 算法笔记  胡凡 / 曾磊\\
$[2]$ 算法导论


\newpage
\section{机器学习问题解答}
\subsection{机器学习-神经网络-深度学习之间的关系}
机器学习的算法之一，是神经网络.\\
神经网络的优化方法之一，是深度学习.

\subsection{神经网络的分类}
· 基础神经网络 —— 全链接层 —— BP神经网络\\
· 卷积神经网络 —— 卷积层,采样层,全链接层 —— 图像处理,计算机视觉  —— LeNet, AlexNet, VGG, GoogLeNet, ResNet...\\
· 循环神经网络 —— 自然语言处理 —— RNN,LSTM\\

\subsection{推荐资料}
· 周志华.机器学习\\
· 莫烦.机器学习教程.https://space.bilibili.com/243821484?from=search&seid=17685821864565324198\\
· 李宏毅-深度学习课件.pdf\\
· 3B1B.神经网络教程.https://www.bilibili.com/video/BV1bx411M7Zx \\
· Github上,有各类网络的具体实现程序.

\subsection{推荐编程库平台}
· pytorch.https://pytorch.org/


\newpage
\section{机器学习 Machine Learning}
\subsection{分类}
\textbf{监督学习}: 提供数据和答案.\\
\textbf{无监督学习}: 只提供数据和答案\\
\textbf{强化学习}: 动态环境中自主学习.



\subsection{数据降维 —— 主成分分析 Principal Component Analysis}
\textbf{目的}: 数据降维.将N维数据集在超平面的投影，作为目标输出数据集.\\
$[1]$ 最近重构性: 样本点到这个超平面的距离都足够近。\\
$[2]$ 最大可分性: 样本点在这个超平面的投影能尽可能分开。\\
\textbf{输入}: x: X维向量集\\
\textbf{输出}: y: Y维向量集,$Y < X$.\\
\textbf{流程}:\\
$[1]$ 数据中心化, $\sum x_i = 0$\\
$[2]$ 计算协方差矩阵 $X X^T$\\
$[3]$ 对协方差特征值分解\\
$[4]$ 取最大的d'个特征值所对应的特征向量${w_1,w_2,...,w_d'}$,投影矩阵$W = {w_1,w_2,...,w_d'}$\\
$[5]$ 样本点在超平面投影: $y_i = W^T x_i$\\
\textbf{推导}:\\
(分别从目的[1,2]可推得相同结果.)\\
若样本点到超空间投影: $y = W^T x$, 尽可能分开.即,方差最大,\
$$\Rightarrow max \sum W^T x x^T W \quad \Rightarrow \max_W\ tr( W^T x x^T W ) \quad ,\quad st. W^T W = I$$
拉格朗日乘子法,得:	
$$\Rightarrow X X^T ω_i = \lambda_i \omega_i$$
即.对协方差$X X_T$ 特征值求解. 取特征值最大的$dimension(y)$个特征向量, 即目标投影矩阵W.


\subsection{数据聚类 —— K-Mean聚类}
\textbf{目的}: 对N维分布的数据点，可以将其聚类在 K 个关键簇内\\
\textbf{输入}: x: N维向量集; K:分类数;\\
\textbf{输出}: 分成K个已分类的向量集.\\
\textbf{流程}:\\
$[1]$ 随机选择 K 个簇心点 Center\\
$[2]$ 迭代开始\\ 
|\quad $[3]$ 归零 Cluster , Cluster: 簇,记录ith簇内的数据指针。\\ 
|\quad $[4]$ 计算每个xi到簇心μj的距离\\
|\quad $[5]$ 选择距离最小的簇心, 将该点加入其簇内\\
|\quad $[6]$ 对每个簇,计算其质心 Center'\\
|\quad $[7]$ $Center \neq Center'$, 则更正Center为 Center'\\
|\quad $[8]$ 迭代重新开始,若一轮无更正时，迭代结束\\
\textbf{推导}:\\


\subsection{自主学习 —— Q-Learning}
\textbf{目的}: 训练:绘制"状态-动作"表.\quad 工作:输入"状态",输出"动作".\\
\textbf{输入}: 动态的游戏过程，游戏的反馈数值.\\
\textbf{输出}: 输入状态下，给出的动作.\\
\textbf{原理}:
$$ Q(s,a) = (1 + lr)·Q(s,a) + lr·( R + g·max Q(s',:) ) $$
s: State \quad a: Action \quad R: Reward \quad lr: Learning Rate \quad g: Forget Factor \\
选择动作:$\epsilon$-greedy方法: 每个状态以$\epsilon$概率随机选取动作，$1 - \epsilon$概率选择当前最优解\\
眼前利益R: 动态环境反馈值.\quad 记忆中的利益 max Q(s',:): 记忆里，新位置s'能给出的最大效用值.\\
forget factor越大，越重视以往经验，越小，则只重视眼前利益R.\\
\textbf{流程}:\\
$[1]$ Init Q table arbitrarily\\
$[2]$ Repeat (for each episode), until s is terminal\\
|\quad $[3]$ Choose a from s using policy derived from Q (eg. ε-greedy)\\
|\quad $[4]$ Take action a, observe r s'\\
|\quad $[5]$ Q(s,a) = (1 + lr)·Q(s,a) + lr·( R + g·max Q(s',:) )\quad s = s'\\
\textbf{Ps.}:
可以逐渐降低随机选取动作的概率ε，一开始随机率可达100\%,然后随训练次数的深入，应当逐渐降低随机概率。


\subsection{自主学习 —— Deep Q Network}
\textbf{目的}: 拟合"状态-动作"对应关系.\quad 工作:输入"状态",输出"动作".\\
\textbf{原理}: Q-Learning + Neural Network\\
Loss Function: 近似值和真实值的均方差\\
\textbf{输入}: 动态的游戏过程，游戏的反馈数值.\\
\textbf{输出}: 输入状态下，给出的动作.\\
\textbf{流程}:\\
$[1]$ Initialize replay memory D to capacity N\\
    Initialize action-value function Q with random weights θ\\
    Initialize target action-value function Q with weights θ- = θ\\
$[2]$ for episode 1,M do\\


\subsection{数据分类 —— 支持向量机 Support Vector Machines }
\textbf{目的}: 数据分类,找到不同类别N维数据点之间的，分割平面.\\
找到目标超平面, 使得[所有样本点间隔最小值γmin = min γi]最大.\\
$$ max_wb( min_i 1/||w||·|w xi + b| ) $$
$$ st.  w x + b > 0 , yi = +1  and  w x + b < 0 , yi = -1  (分类) $$
特征空间上, \textbf{间隔最大}的线性分类器.\\
超平面: y = w x + b\\
间隔: 样本点, 到超平面的距离.\\
        γi = 1/||w||·|w x + b|	(点到面距离公式)\\
样本点: {(xi,yi)} i=1toN    xi∈R_n 实向量    yi∈{-1,1}\\
\\
\textbf{输入}: \\
\textbf{输出}: \\
\textbf{流程}:
$[1]$ 选择惩罚参数 C > 0, 构造并求解凸二次规划问题, 得到最优解α\\
$[2]$ w = Σi αi yi xi    b = yj - Σαi yi (xi·xj)\\
$[3]$ 得到分离超平面 w x + b = 0\\
    分类决策函数: f(x) = sign( w x + b )
\textbf{推导}:
=>	|w xi + b| = yi(w xi + b)		//去绝对值\\
=>	max_wb( 1/||w||·min_i yi(w xi + b) )    st. yi(w x + b) > 0\\
转化为:\\
    min_wb 1/2·||w||²    st. yi (W x + b) ≥ 1\\
    凸二次规划问题, 用拉格朗日乘子法, 得其对偶问题.
\\
    min 1/2·Σi Σj αi αj yi yj K(xi,xj) - Σi αi	//线性时 K(xi,xj)即内积\\
    st. Σi αi yi = 0    0≤αi≤C\\
Kernal Trick: 升到高维, 实现非线性分类\\
Classical Kernal Function:\\
    * 高斯核函数: K(x,z) = exp( -||x - z||² / 2σ² )\\
\\
二次规划优化问题:\\
算法: Sequential Minimal Optimization 算法\\
思路: 若所有变量解都满足此最优化问题的KKT条件，则得到最优化问题解\\
Karush Kuhn Tucker条件: 非线性规划最佳解的必要条件


\subsection{数据分类 —— 感知机}
\textbf{目的}: 数据分类,找到不同类别N维数据点之间的，分割平面.
· 神经网络的前身
\textbf{输入}: \\
\textbf{输出}: \\


\subsection{数据分类 —— 神经网络 Neural Networks}
\textbf{目的}: 数据分类,找到不同类别N维数据点之间的，分割平面.\\
\textbf{输入}: \\
\textbf{输出}: \\

\subsubsection{神经网络层 Neural Layer}
\textbf{正向}: 
$$ y = \sigma( \sum w_i·x_i + b ),\quad (Matrix): y = \sigma(W x + b)$$\\
$x_i$: 输入 \quad $y$: 输出 \quad $\sum w_i·x_i + b$: 线性拟合 \quad $\sigma()$: 激活函数, 使线性拟合非线性化, $eg. relu(x), Sigmoid(x)$\\
\textbf{误差}: 
$$E_{total} = \sum (target_i - output_i)^2$$
\textbf{反向}: (Matrix)\\
\begin{displaymath}
    \left\{ \begin{array}{ll}
    \frac{\partial E}{\partial w_L} = \delta L·y_{L-1}' \quad \frac{\partial E}{\partial b_L} = \delta L  \\
    \delta L = (w_{L+1}'\delta_{L+1}) \odot (\delta'z_L) \quad \delta L_{out} = \nabla aE \odot (\delta'z_{Lout})
    \end{array} \right.
\end{displaymath}
zi = Σwi·xi + b \quad yi = σ(zi) \quad $E$: is E_total	outL: output layer \quad , \quad $\odot$: 逐元素乘法 \quad , \quad $L$: Lth layer \quad $wL_jk$: the kth weight of jth Neuron in Lth layer\\
\textbf{目的}: 
$$wL_new = wL + lr·\partial E/\partial wL, to get \partial E/\partial wL \quad (and bL)$$
\textbf{链式法则}:
$$\partial E/\partial wL_jk = \partial E/\partial yL_j·\partial yL_j/\partial zL_j·\partial zL_j/\partial wL_jk	(and bL)$$
\textbf{推导}:\\
输出层: L = outL\\
\textbf{1} $\partial E/\partial yL_j = \partial (Σ(target_i - yL_i)²)/\partial yL = -2(target_j - yL_j)$\\
\textbf{2} $\partial yL/\partial zL_j = σ'(zL_j)$\\
\textbf{3} $\partial zL/\partial wL_jk = y_L-1_j$ \\
the amount that a small nudge to this weight influences the last layer depends on how strong the previous neuron is.\\
$$ \Rightarrow \partial E/\partial wL_jk = -2(target_j - yL_j)·σ'(zL_j)·y_L-1_k = δL_j·y_L-1_k = δL·y_L-1'$$
$δL_j = σ'(zL_j)·-2(target_j - yL_j) = \partial E/\partial zL_j$\\
$for bL: \partial zL_j/\partial bL_j = 1		∴\partial E/\partial bL_j = δL_j$\\
隐藏层:\\
\textbf{1} $\partial E/\partial yL_j = Σ\partial E_oi/\partial yL_j$\\
$\partial E_oi/\partial yL_j = \partial E_oi/\partial zL_j·\partial zL_j/\partial yL_j = δ_L+1_j·\partial (Σwi·xi + b)/\partial yL_j = δo1·wL_jk$\\
$\partial E/\partial yL_j = Σδoi·wL_jk$\\
\textbf{2][3} $\partial yL/\partial zL_j, \partial zL/\partial wL_jk$ 同上\\
\textbf{∴} $\partial E/\partial wL_jk = -2(target_j - yL_j)·σ'(zL_j)·y_L-1_j$\\
\textbf{ReLU}: ReLU(x) = x > 0 ? x : 0\\
\textbf{Sigmoid}: Sigmoid(x) = 1 / (1 + e^-x)\\

\subsubsection{卷积层 Convolution Layer}
kernel: 卷积核 \quad , \quad padding: 加边框宽度 \quad , \quad in/outChannelNum: 输入/输出通道数\\
\textbf{}:
$$ Height_out = (Height_in - Height_kernel + 2 * padding) / (stride + 1) $$
$$ Width_out = (Width_in - Width_kernel + 2 * padding) / (stride + 1) $$
\textbf{正向}: 卷积操作\\
	
\subsubsection{采样层 Pool Layer}
\textbf{分类}:	$[1]$ AvePool 平均采样层 \quad , \quad $[2]$ MaxPool 最大采样层

\subsubsection{神经网络分类}
$[1]$ 基础神经网络 —— 全链接层 —— BP神经网络\\
$[2]$ 卷积神经网络 —— 卷积层,采样层,全链接层 —— 图像处理,计算机视觉 —— LeNet, AlexNet, VGG, GoogLeNet, ResNet...\\
$[3]$ 循环神经网络 —— 自然语言处理 —— RNN,LSTM\\

\subsubsection{LeNet网络}
\subsubsection{Inception 模块}
\subsubsection{GoogLeNet网络}
\subsubsection{LSTM}
\subsubsection{经典数据集：Minst手写数字}

\subsection{异常检测 —— }



\section{Reference}
$[1]$ 周志华.机器学习

\end{document}